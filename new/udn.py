# -*- coding: utf-8 -*-
"""
Created on Wed Oct 16 21:23:02 2024

@author: USER
"""

# åŸå¸‚åç¨±:udn.py
# ===============================
# è¯åˆæ–°èç¶²çˆ¬èŸ²ç¨‹å¼
# UDN News Scraper
# ===============================

import requests  # åŒ¯å…¥ requests æ¨¡çµ„ï¼Œç”¨æ–¼ç™¼é€ HTTP è«‹æ±‚
from flask import Blueprint, jsonify
from models import db, NewsArticle  # å¼•å…¥è³‡æ–™åº«èˆ‡æ–°èæ¨¡å‹
from database import db
from dateutil import parser  # å®‰è£ä¾è³´åº«ï¼špip install python-dateutil
from datetime import datetime  # è§£ææ™‚é–“æ ¼å¼
import random  # ç”¨æ–¼éš¨æ©Ÿå…§æ–‡ç”Ÿæˆ

# åˆå§‹åŒ– Blueprint
udn_bp = Blueprint('udn', __name__, url_prefix="/udn")

def fetch_udn_news():
    """
    å¾è¯åˆæ–°èç¶² API ç²å–å³æ™‚æ–°è
    Fetch real-time news from UDN News API
    """
    # è¨­å®š API ç›®æ¨™ç¶²å€
    api_url = "https://udn.com/api/more?page=2&id=&channelId=1&cate_id=0&type=breaknews&totalRecNo=20752"
    headers = {
        "User-Agent": "Mozilla/5.0"  # æ¨¡æ“¬ç€è¦½å™¨è«‹æ±‚
    }

    # ç™¼é€ API è«‹æ±‚
    response = requests.get(api_url, headers=headers)
    if response.status_code != 200:
        print(f"âŒ ç„¡æ³•ç²å–æ–°èè³‡æ–™ï¼ŒHTTP ç‹€æ…‹ç¢¼ï¼š{response.status_code}")
        return []

    try:
        news_data = response.json()  # å°‡ API å›æ‡‰è½‰ç‚º JSON
    except Exception as e:
        print(f"âŒ JSON è§£æå¤±æ•—ï¼š{e}")
        return []

    inserted_news = []  # å„²å­˜æˆåŠŸæ’å…¥çš„æ–°è
    source = "UDN"  # æ–°èä¾†æº
    random_texts = ["å‰å¾€è§€çœ‹", "æ·±å…¥ç­è§£", "ä¾†å»çœ‹çœ‹"]  # éš¨æ©Ÿå…§æ–‡

    # è™•ç†æ¯ç­†æ–°èè³‡æ–™
    for index, news in enumerate(news_data.get("lists", []), start=1):
        title = news.get("title", "ç„¡æ¨™é¡Œ")
        link = "https://udn.com" + news.get("url", "")
        content = news.get("content") or random.choice(random_texts)
        photo = "https://example.com/default-image.png"  # é è¨­åœ–ç‰‡
        published_at = news.get("time", None)

        if published_at:
            try:
                published_at = parser.parse(published_at)
            except Exception:
                published_at = datetime.utcnow()
        else:
            published_at = datetime.utcnow()

        # è³‡æ–™åº«æŸ¥é‡èˆ‡æ’å…¥
        existing = NewsArticle.query.filter_by(url=link).first()
        if not existing:
            news_article = NewsArticle(
                title=title,
                content=content,
                source=source,
                image_url=photo,
                url=link,
                published_at=published_at
            )
            db.session.add(news_article)
            db.session.commit()
            inserted_news.append({"title": title, "link": link})

    return inserted_news


@udn_bp.route("/scrape", methods=["GET"])
def fetch_udn_api():
    """
    æä¾› APIï¼Œæ‰‹å‹•è§¸ç™¼æ–°èçˆ¬å–
    Provide an API for manually triggering news scraping
    """
    news = fetch_udn_news()
    return jsonify({"message": f"æˆåŠŸå­˜å…¥ {len(news)} ç¯‡æ–°è", "data": news}), 200


"""
ç¨‹å¼åŸç†ç¸½çµï¼š
1. æœ¬ç¨‹å¼ä½¿ç”¨ requests æ¨¡çµ„å‘è¯åˆæ–°èç¶²çš„æŒ‡å®š API ç›®æ¨™ç¶²å€ç™¼é€ GET è«‹æ±‚ï¼Œä¸¦ä»¥ UTF-8 ç·¨ç¢¼è§£æå›æ‡‰çš„ HTML/JSON è³‡æ–™ã€‚
2. ä½¿ç”¨ BeautifulSoup è§£æéƒ¨åˆ† HTMLï¼ˆé›–ç„¶æ­¤ç¨‹å¼ä¸»è¦å¾ API å–å¾— JSON æ•¸æ“šï¼‰ï¼Œä¸¦å°‡æ•¸æ“šå­˜å…¥ SQLite è³‡æ–™åº«ä¸­ã€‚è³‡æ–™åº«é€£ç·šä½¿ç”¨ sqlite3 æ¨¡çµ„å»ºç«‹ï¼Œä¸¦å…ˆå‰µå»ºä¸€å€‹åç‚º news çš„è³‡æ–™è¡¨ï¼Œè©²è¡¨åŒ…å«æ–°èæ¨™é¡Œã€é€£çµèˆ‡æ’å…¥æ™‚é–“ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰ã€‚
3. ç¨‹å¼é€ç­†è™•ç† API å›å‚³çš„æ–°èæ•¸æ“šï¼Œå¾æ¯ç­†è³‡æ–™ä¸­æå–æ¨™é¡Œèˆ‡ URLï¼Œä¸¦å°‡ URL è£œå…¨ç‚ºå®Œæ•´çš„é€£çµã€‚
4. åœ¨å°‡æ–°èæ•¸æ“šæ’å…¥è³‡æ–™åº«ä¹‹å‰ï¼Œå…ˆåˆ©ç”¨ SQL æŸ¥è©¢æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ–°èï¼ˆæ ¹æ“šé€£çµåˆ¤æ–·ï¼‰ï¼Œä»¥é¿å…é‡è¤‡æ’å…¥ã€‚
5. æˆåŠŸæ’å…¥æ–°æ–°èå¾Œï¼Œç¨‹å¼æäº¤äº¤æ˜“ä¸¦è¼¸å‡ºæç¤ºï¼›è‹¥æ–°èå·²å­˜åœ¨ï¼Œå‰‡è¼¸å‡ºå·²å­˜åœ¨çš„æç¤ºè¨Šæ¯ã€‚
6. æœ€å¾Œï¼Œç¨‹å¼é—œé–‰è³‡æ–™åº«é€£ç·šï¼Œå®Œæˆæ–°èè³‡æ–™çš„çˆ¬å–èˆ‡å„²å­˜æµç¨‹ã€‚
"""

# ===========================================================
# è¨»è§£å€ï¼ˆèˆŠç‰ˆç¨‹å¼ç¢¼ï¼‰
# Code Comment Section (Replaced code from previous version)
# ===========================================================

"""
    ### è¢«æ›¿æ›çš„ç¨‹å¼ç¢¼éƒ¨åˆ† ###
    1. **SQLite è³‡æ–™åº«æ“ä½œ**ï¼š
       åŸæœ¬ç¨‹å¼ä½¿ç”¨ SQLite é€²è¡Œè³‡æ–™å­˜å–ï¼Œå·²è¢«æ›¿æ›ç‚º SQLAlchemy å’Œ MySQLã€‚
       ä»¥ä¸‹ç‚ºèˆŠç¨‹å¼ç¢¼ï¼š
       
       # conn = sqlite3.connect("udn_news.db")  # å»ºç«‹æˆ–é€£æ¥ SQLite è³‡æ–™åº«
       # cursor = conn.cursor()
       # cursor.execute("" "
       #     CREATE TABLE IF NOT EXISTS news (
       #         id INTEGER PRIMARY KEY AUTOINCREMENT,
       #         platform TEXT,
       #         title TEXT UNIQUE,
       #         link TEXT,
       #         created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
       #     )
       # "" ")
       # conn.commit()
       # try:
       #     cursor.execute("INSERT INTO news (title, link) VALUES (?, ?)", (title, link))
       #     conn.commit()
       # except sqlite3.IntegrityError:
       #     print(f"âš ï¸ é‡è¤‡æ–°èï¼š{title}")
    
    2. **ç›´æ¥åˆ—å°æ–°èå…§å®¹**ï¼š
       åŸç¨‹å¼ç›´æ¥åˆ—å°æ¯å‰‡æ–°èï¼Œæœªæ•´åˆè‡³è³‡æ–™åº«ã€‚
       ä»¥ä¸‹ç‚ºèˆŠé‚è¼¯ï¼š
       
       # for index, news in enumerate(news_data.get("lists", []), start=1):
       #     print(f"{index}. {news.get('title', 'ç„¡æ¨™é¡Œ')}")
       #     print(f"   ğŸ”— {news.get('url')}")

"""