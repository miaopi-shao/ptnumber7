import requests
from bs4 import BeautifulSoup
import random
from flask import Blueprint, jsonify

# å®šç¾© Blueprintï¼Œæä¾›åœ‹éš›æ–°èçš„ API è·¯ç”±
international_news_bp = Blueprint("international_news", __name__)

# çˆ¬å– BBC æ–°è
import requests
from bs4 import BeautifulSoup
import random

def fetch_bbc_news():
    url = "https://www.bbc.com/news"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # æ‰¾åˆ°çˆ¶å…ƒç´ 
    articles = soup.find_all("div", class_="sc-c6f6255e-0 eGcloy")

    # **ğŸ”¥ å…ˆç¯©é¸å‡ºæœ‰åœ–ç‰‡çš„æ–°è**
    filtered_articles = []
    for article in articles:
        image_tag = article.find("img", class_="sc-a34861b-0 efFcac")
        image_link = image_tag["src"] if image_tag and image_tag.get("src") else None

        if image_link:  # åªæœ‰æœ‰åœ–ç‰‡çš„æ–°èæ‰åŠ å…¥
            filtered_articles.append(article)

    # **ğŸ”¥ å†å¾æœ‰åœ–ç‰‡çš„æ–°èéš¨æ©Ÿé¸æ“‡å…©å‰‡**
    selected_articles = random.sample(filtered_articles, min(2, len(filtered_articles))) if filtered_articles else []

    # çµ„åˆçµæœ
    results = []
    for article in selected_articles:
        title_tag = article.find("h2")
        title = title_tag.text.strip() if title_tag else "ç„¡æ¨™é¡Œ"

        summary_tag = article.find("p")
        summary = summary_tag.text.strip() if summary_tag else "Maybe you should check it out ?"

        link_tag = article.find("a")
        link = "https://www.bbc.com" + link_tag["href"] if link_tag and link_tag.get("href") else "#"

        results.append({
            "title": title,
            "link": link,
            "summary": summary,
            "image_link": image_link,  # ğŸ”¥ ç¢ºä¿åœ–ç‰‡å·²å­˜åœ¨
            "source": "BBC",
        })

    return results if results else [{"title": "ç„¡æ³•ç²å– BBC æ–°è", "link": "#", "summary": "è«‹ç¨å¾Œå†è©¦", "image_link": "https://via.placeholder.com/150", "source": "BBC"}]

# çˆ¬å– Al Jazeera æ–°è
def fetch_aljazeera_news():
    url = "https://www.aljazeera.com/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # æ‰¾åˆ°æ–°èå€å¡Š
    articles = soup.find_all("article", class_="gc u-clickable-card gc--type-post gc--with-image")
    # print("âœ… æ‰¾åˆ°:", articles)
    # print("âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ… ")

     # **ğŸ”¥ å…ˆç¯©é¸å‡ºæœ‰åœ–ç‰‡çš„æ–°è**
    filtered_articles = []
    for article in articles:
        image_tag = article.find("img", class_="article-card__image gc__image")
        # print("âœ… æ‰¾åˆ°åœ–ç‰‡:", image_tag)  # Debug: ç¢ºèªåœ–ç‰‡è§£ææˆåŠŸ
        # print("âœ…âœ…---------âœ…âœ… ")

        # **ğŸ”¥ ä½¿ç”¨ `srcset` æˆ– `src` ä¾†ç²å–åœ–ç‰‡**
        if image_tag and image_tag.has_attr("src"):
            image_link = image_tag["src"]
        else:
            image_link = None

        # æ‹¼æ¥å®Œæ•´åœ–ç‰‡ç¶²å€
        if image_link and image_link.startswith("/"):
            image_link = "https://www.aljazeera.com" + image_link
            # print(image_link)
            # print("âœ…-âœ…-âœ…-âœ…-âœ… ")
        # æ­¤è™•æ‡‰è©²æ·»åŠ :
        if image_link:
            filtered_articles.append((article, image_link))

    # **ğŸ”¥ å¾æœ‰åœ–ç‰‡çš„æ–°èéš¨æ©Ÿé¸æ“‡å…©å‰‡**
    selected_articles = random.sample(filtered_articles, min(2, len(filtered_articles))) if filtered_articles else []
    # print("-----æ‰¾åˆ°:---", selected_articles,"-----------")
    # print("XXXXXXXXXXXXXXXXXXX ")
    

    # **ğŸ”¥ çµ„åˆçµæœ**
    results = []
    for article, image_link in selected_articles:
        title_tag = article.find("h3", class_="gc__title")
        title = title_tag.text.strip() if title_tag else "ç„¡æ¨™é¡Œ"
        # print("âœ… æ‰¾åˆ°æ¨™é¡Œ:", title)  # Debug: ç¢ºèªæ¨™é¡Œè§£ææˆåŠŸ
        
        link_tag = article.find("a", class_="u-clickable-card__link")
        link = "https://www.aljazeera.com" + link_tag["href"] if link_tag and link_tag.get("href") else "#"
        # print("âœ… æ‰¾åˆ°é€£çµ:", link)  # Debug: ç¢ºèªæ–°èé€£çµè§£ææˆåŠŸ
        
        summary_tag = article.find("p", class_="gc__excerpt")
        summary = summary_tag.text.strip() if summary_tag else "Let's take a look at this report !"
        # print("âœ… æ‰¾åˆ°æ‘˜è¦:", summary)  # Debug: ç¢ºèªæ‘˜è¦è§£ææˆåŠŸ
        
        
        results.append({
            "title": title,
            "link": link,
            "summary": summary,
            "image_link": image_link,  
            "source": "Al Jazeera",
        })
    # print("ğŸ”¥ æœ€çµ‚æ–°èåˆ—è¡¨:", results)  # Debug: ç¢ºèª `results` åˆ—è¡¨ä¸ç‚ºç©º

    return results if results else [{"title": "ç„¡æ³•ç²å– Al Jazeera æ–°è", "link": "#", "summary": "è«‹ç¨å¾Œå†è©¦", "image_link": "https://via.placeholder.com/150", "source": "Al Jazeera"}]

# æ•´åˆéš¨æ©ŸæŠ“å–å„å…©å‰‡æ–°è
def fetch_international_news():
    # å¾ BBC å’Œ Al Jazeera å„æŠ“å–å…©å‰‡æ–°è
    bbc_news = fetch_bbc_news()
    aljazeera_news = fetch_aljazeera_news()
    return aljazeera_news + bbc_news

# API è·¯ç”±ï¼Œæä¾›åœ‹éš›æ–°èè³‡æ–™
@international_news_bp.route("/scrape", methods=["GET"])
def fetch_news_api():
    """ æä¾› APIï¼Œè¿”å›éš¨æ©Ÿå„å…©å‰‡åœ‹éš›æ–°è """
    try:
        news = fetch_international_news()
        return jsonify({"message": "æˆåŠŸæŠ“å–åœ‹éš›æ–°è", "data": news}), 200
    except Exception as e:
        return jsonify({"error": f"æŠ“å–æ–°èå¤±æ•—: {str(e)}"}), 500

# # é‹è¡Œç¨‹å¼ä¸¦æ‰“å°çµæœ
# if __name__ == '__main__':
#     news = fetch_international_news()
#     print("=== éš¨æ©ŸæŠ“å–å„å…©å‰‡æ–°è ===")
#     for idx, article in enumerate(news, start=1):
#         print(f"æ–°è {idx}:")
#         print(f"æ¨™é¡Œ: {article['title']}")
#         print(f"é€£çµ: {article['link']}")
#         print(f"æ‘˜è¦: {article['summary']}")
#         print(f"åœ–ç‰‡: {article['image_link']}")
#         print(f"åœ–ç‰‡: {article['source']}")
#         print("==============================")